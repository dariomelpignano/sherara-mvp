EU AI Act - Key Requirements for AI Systems

Risk-Based Approach
The AI Act establishes a risk-based regulatory framework with four categories: unacceptable risk (prohibited), high risk (strict requirements), limited risk (transparency obligations), and minimal risk (voluntary codes). Organizations must assess and categorize their AI systems accordingly.

Prohibited AI Practices
AI systems are prohibited that: deploy subliminal techniques causing harm, exploit vulnerabilities of specific groups, enable social scoring by public authorities, or use real-time remote biometric identification in public spaces for law enforcement (with limited exceptions).

High-Risk AI Systems - Definition
High-risk AI systems include those used in: critical infrastructure, educational/vocational training, employment and worker management, essential services access, law enforcement, migration and border control, justice administration, and democratic processes. Systems must undergo conformity assessment before market placement.

Requirements for High-Risk AI Systems
High-risk AI systems must implement: risk management systems throughout lifecycle, data governance ensuring training data quality, technical documentation before market placement, automatic logging capabilities, transparency and information provision to users, human oversight measures, and accuracy/robustness/cybersecurity standards.

Data Governance and Management
Training, validation and testing datasets must be relevant, representative, free of errors and complete. Data collection and processing must comply with applicable laws. Special attention required for personal data processing and bias mitigation.

Technical Documentation
Comprehensive documentation required including: general system description, detailed algorithm information, development process description, risk assessment results, validation and testing procedures, and post-market monitoring plans. Documentation must be kept for 10 years.

Transparency and Information Provision
Users must be informed that they are interacting with an AI system. Clear information about capabilities and limitations required. Instructions for use must enable appropriate human oversight. Providers must ensure sufficient transparency for users to interpret system output.

Human Oversight
High-risk AI systems must be designed for effective human oversight. Human operators must: fully understand system capabilities and limitations, be able to correctly interpret outputs, be able to decide not to use the system, and be able to intervene or stop the system.

Accuracy, Robustness and Cybersecurity
Systems must achieve appropriate levels of accuracy as declared in instructions. Performance metrics must be provided. Systems must be resilient against errors, faults and inconsistencies. Appropriate cybersecurity measures required throughout lifecycle.

Quality Management System
Providers of high-risk AI systems must establish quality management covering: regulatory compliance strategy, techniques for design and quality control, development/quality assurance procedures, examination/testing procedures, data management systems, and post-market monitoring system.

Conformity Assessment Procedures
High-risk AI systems require conformity assessment before market placement. This may involve internal control or third-party assessment depending on system type. Assessment covers compliance with all applicable requirements.

CE Marking
High-risk AI systems must bear CE marking indicating conformity with requirements. Marking must be visible, legible and indelible. Accompanied by EU declaration of conformity with required information.

Registration Obligations
High-risk AI systems must be registered in EU database before placement on market. Registration includes: provider identification, system description, intended purpose, and conformity status. Database publicly accessible with some information restricted.

Post-Market Monitoring
Providers must establish and document post-market monitoring system proportionate to risk. System must actively collect and review experience from use, identify need for corrective actions, and ensure continuous compliance.

Incident Reporting
Providers must report serious incidents and malfunctions to relevant authorities. Reports required within 15 days of awareness. Must include incident description, corrective actions taken, and risk assessment.

Obligations for Providers
Providers must: ensure conformity with requirements, have quality management system, conduct conformity assessment, draw up technical documentation, maintain logs, ensure human oversight capability, and register system in EU database.

Obligations for Deployers
Deployers must: use systems according to instructions, ensure human oversight, monitor operation for risks, inform provider/authorities of incidents, conduct DPIA where required, and use logs for compliance verification.

General Purpose AI Models
Providers of general-purpose AI models must: provide technical documentation, provide information for downstream providers, establish copyright compliance policy, and publish summary of training data. Additional requirements for systemic risk models.

Codes of Conduct
Commission and Member States encourage drawing up codes of conduct for non-high-risk AI systems. Codes should foster voluntary application of requirements and additional commitments such as environmental sustainability.

Penalties
Penalties for non-compliance range up to €35 million or 7% of total worldwide annual turnover for prohibited practices, and up to €15 million or 3% for other violations. Providers of foundational models face specific penalty structure.

Innovation Support
Regulatory sandboxes established to facilitate development and testing of innovative AI systems. Priority access for SMEs and startups. Reduced fees and administrative burden for small-scale providers.
